{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 19:45:02 WARN Utils: Your hostname, abanoub-Latitude-E5440 resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface wlp2s0)\n",
      "25/03/16 19:45:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/03/16 19:45:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#creating spark session and importing libraries\n",
    "import findspark\n",
    "findspark.init(\"/opt/spark\")  \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as fn\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySQL Integration\") \\\n",
    "    .config(\"spark.jars\", \"mysql-connector-j-8.0.33.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"mysql-connector-j-8.0.33.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#reading all files from the archive \n",
    "df_total=spark.read.json('archive/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+--------------------+---------+----------------+-----------+--------------------+-----+-----------+--------------------+------------+-------------------+\n",
      "|            created|         description|forks|           full_name|       id|        language|open_issues|           repo_name|stars|subscribers|              topics|        type|           username|\n",
      "+-------------------+--------------------+-----+--------------------+---------+----------------+-----------+--------------------+-----+-----------+--------------------+------------+-------------------+\n",
      "|2014-02-25 08:00:08|Apache Spark - A ...|25357|        apache/spark| 17165658|           Scala|        242|               spark|32296|       2080|[python, scala, r...|Organization|             apache|\n",
      "|2017-08-09 19:39:59|Distributed train...| 2027|     horovod/horovod| 99846383|          Python|        298|             horovod|12219|        334|[tensorflow, uber...|Organization|            horovod|\n",
      "|2014-08-08 07:30:51|Notes talking abo...| 1773|JerryLead/SparkIn...| 22749672|            NULL|         27|      SparkInternals| 4774|        619|                  []|        User|          JerryLead|\n",
      "|2019-04-22 18:56:51|An open-source st...|  985|      delta-io/delta|182849188|           Scala|        180|               delta| 4164|        188|[spark, acid, big...|Organization|           delta-io|\n",
      "|2017-01-20 18:15:57|TensorFlowOnSpark...|  966|yahoo/TensorFlowO...| 79584587|          Python|          6|   TensorFlowOnSpark| 3768|        286|[tensorflow, spar...|Organization|              yahoo|\n",
      "|2019-01-03 21:46:54|Koalas: pandas AP...|  333|   databricks/koalas|164026325|          Python|        103|              koalas| 3095|        222|[spark, pandas, p...|Organization|         databricks|\n",
      "|2014-08-21 23:07:47|REST job server f...| 1003|spark-jobserver/s...| 23205911|           Scala|        106|     spark-jobserver| 2767|        219|[spark, rest-api,...|Organization|    spark-jobserver|\n",
      "|2017-05-05 02:27:30|Distributed Tenso...|  714|intel-analytics/a...| 90328920|Jupyter Notebook|        560|       analytics-zoo| 2483|        102|[apache-spark, de...|Organization|    intel-analytics|\n",
      "|2019-07-04 17:09:41|Distributed compu...|  148|ballista-compute/...|195277793|            Rust|          0|            ballista| 2285|         71|[rust, arrow, dat...|Organization|   ballista-compute|\n",
      "|2018-08-07 20:55:14|Deequ is a librar...|  389|       awslabs/deequ|143925946|           Scala|        103|               deequ| 2158|         70|[dataquality, spa...|Organization|            awslabs|\n",
      "|2017-11-02 16:15:15|TransmogrifAI (pr...|  374|salesforce/Transm...|109289451|           Scala|         44|       TransmogrifAI| 2101|        146|[ml, automl, tran...|Organization|         salesforce|\n",
      "|2019-10-22 19:13:09|A new arguably fa...|  179|     rajasekarv/vega|216890889|            Rust|         34|                vega| 2028|        116|                  []|        User|         rajasekarv|\n",
      "|2017-05-31 17:30:28|Deep Learning Pip...|  494|databricks/spark-...| 92971378|          Python|         85| spark-deep-learning| 1912|        151|                  []|Organization|         databricks|\n",
      "|2018-01-03 17:43:16|Kubernetes operat...|  921|GoogleCloudPlatfo...|116165188|              Go|        327|spark-on-k8s-oper...| 1895|         82|[kubernetes, kube...|Organization|GoogleCloudPlatform|\n",
      "|2014-07-25 20:08:44|Oryx 2: Lambda ar...|  412|    OryxProject/oryx| 22269384|            Java|          1|                oryx| 1796|        214|[lambda-architect...|Organization|        OryxProject|\n",
      "|2019-04-22 18:55:55|.NET for ApacheÂ® ...|  270|        dotnet/spark|182849051|              C#|        140|               spark| 1746|         82|[spark, csharp, d...|Organization|             dotnet|\n",
      "|2015-10-08 12:19:32|Apache Spark dock...|  593|big-data-europe/d...| 43886361|           Shell|         38|        docker-spark| 1706|         99|[spark-kubernetes...|Organization|    big-data-europe|\n",
      "|2015-08-22 13:52:08|Elassandra = Elas...|  204|strapdata/elassandra| 41209174|            Java|         40|          elassandra| 1624|         87|[cassandra, elast...|Organization|          strapdata|\n",
      "|2015-05-06 07:45:21|Apache Spark & Py...|  893|jadianes/spark-py...| 35145949|Jupyter Notebook|          9|  spark-py-notebooks| 1439|         99|[spark, python, p...|        User|           jadianes|\n",
      "|2016-06-28 07:00:06|High performance ...|  697|   apache/carbondata| 62117818|           Scala|        138|          carbondata| 1309|        129|[scala, java, big...|Organization|             apache|\n",
      "+-------------------+--------------------+-----+--------------------+---------+----------------+-----------+--------------------+-----+-----------+--------------------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#testing the reading \n",
    "df_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|         language|\n",
      "+-----------------+\n",
      "|               C#|\n",
      "|            Terra|\n",
      "|             Hack|\n",
      "|         Makefile|\n",
      "|              VBA|\n",
      "|             Cuda|\n",
      "|         Markdown|\n",
      "|              Arc|\n",
      "|             LLVM|\n",
      "|            Metal|\n",
      "|             Haxe|\n",
      "|             Rust|\n",
      "|       JavaScript|\n",
      "|       Emacs Lisp|\n",
      "|             TSQL|\n",
      "|             Perl|\n",
      "|           Puppet|\n",
      "|           Erlang|\n",
      "|          Crystal|\n",
      "|            Jinja|\n",
      "|              Nim|\n",
      "|              C++|\n",
      "|               F#|\n",
      "|           Groovy|\n",
      "|              TeX|\n",
      "|            OCaml|\n",
      "|             Dart|\n",
      "|             MQL5|\n",
      "|            Cirru|\n",
      "|              Elm|\n",
      "|              Vue|\n",
      "|              Coq|\n",
      "|       Dockerfile|\n",
      "|     ActionScript|\n",
      "|            Julia|\n",
      "| Jupyter Notebook|\n",
      "|             XSLT|\n",
      "|       Vim script|\n",
      "|    Objective-C++|\n",
      "|                C|\n",
      "|      Objective-C|\n",
      "|          Verilog|\n",
      "|       ApacheConf|\n",
      "|            Frege|\n",
      "|            Swift|\n",
      "|       TypeScript|\n",
      "|             NASL|\n",
      "|              CSS|\n",
      "|            CMake|\n",
      "|              Lex|\n",
      "|           Elixir|\n",
      "|              HCL|\n",
      "|              Nix|\n",
      "|          Haskell|\n",
      "|        Batchfile|\n",
      "|          Jsonnet|\n",
      "|             HTML|\n",
      "|         PigLatin|\n",
      "|         Solidity|\n",
      "|       PowerShell|\n",
      "|                R|\n",
      "|Visual Basic .NET|\n",
      "|            Scala|\n",
      "|          Nearley|\n",
      "|        SaltStack|\n",
      "|          Clojure|\n",
      "|           Reason|\n",
      "|              PHP|\n",
      "|           Kotlin|\n",
      "|             Roff|\n",
      "|           Racket|\n",
      "|               Go|\n",
      "|      WebAssembly|\n",
      "|              Lua|\n",
      "|             Ruby|\n",
      "|           Python|\n",
      "|     CoffeeScript|\n",
      "|          PLpgSQL|\n",
      "|Open Policy Agent|\n",
      "|         Assembly|\n",
      "|           Pascal|\n",
      "|             Java|\n",
      "|            Shell|\n",
      "|   RobotFramework|\n",
      "|             Less|\n",
      "| Rich Text Format|\n",
      "|         Starlark|\n",
      "|              SMT|\n",
      "|         ReScript|\n",
      "|             SCSS|\n",
      "|         AsciiDoc|\n",
      "|           Smarty|\n",
      "|             Riot|\n",
      "|           Matlab|\n",
      "|              QML|\n",
      "|       Inno Setup|\n",
      "|           Svelte|\n",
      "|           MATLAB|\n",
      "|             VimL|\n",
      "|             YARA|\n",
      "|           Nimrod|\n",
      "|              Red|\n",
      "|          BitBake|\n",
      "|              EJS|\n",
      "|            Bicep|\n",
      "|        PureBasic|\n",
      "|         Nunjucks|\n",
      "|             GLSL|\n",
      "|      Mathematica|\n",
      "|       AGS Script|\n",
      "|            Astro|\n",
      "|             HaXe|\n",
      "|            Smali|\n",
      "|         Mustache|\n",
      "|     OpenEdge ABL|\n",
      "|             Twig|\n",
      "|            Dhall|\n",
      "|       PostScript|\n",
      "|              ASP|\n",
      "|           Cython|\n",
      "|            Logos|\n",
      "|               F*|\n",
      "|       Processing|\n",
      "|    SystemVerilog|\n",
      "|             Sass|\n",
      "|      AppleScript|\n",
      "|             SWIG|\n",
      "|      Common Lisp|\n",
      "|              Pug|\n",
      "|            Stata|\n",
      "|       PureScript|\n",
      "|             Pony|\n",
      "|              Max|\n",
      "|           Stylus|\n",
      "|            Forth|\n",
      "|             NULL|\n",
      "+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_total.select('language').distinct().show(136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating view for first requistion (programming_lang) \n",
    "df_total.createOrReplaceTempView('lang')\n",
    "programming_lang=spark.sql('Select language ,count(*) as number_of_repos From lang GROUP BY language')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|  language|number_of_repos|\n",
      "+----------+---------------+\n",
      "|        C#|            336|\n",
      "|     Terra|              1|\n",
      "|      Hack|              1|\n",
      "|  Makefile|             36|\n",
      "|       VBA|              3|\n",
      "|      Cuda|             12|\n",
      "|  Markdown|              3|\n",
      "|       Arc|              1|\n",
      "|      LLVM|              2|\n",
      "|     Metal|              1|\n",
      "|      Haxe|              4|\n",
      "|      Rust|            165|\n",
      "|JavaScript|           5293|\n",
      "|Emacs Lisp|             18|\n",
      "|      TSQL|              9|\n",
      "|      Perl|             14|\n",
      "|    Puppet|              4|\n",
      "|    Erlang|              6|\n",
      "|      NULL|           1424|\n",
      "|   Crystal|              2|\n",
      "+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- number_of_repos: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test 1st df \n",
    "programming_lang.show()\n",
    "programming_lang.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/16 19:45:17 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "[Stage 9:===================>                                       (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|         username|stars|\n",
      "+-----------------+-----+\n",
      "|facebookincubator|  375|\n",
      "|facebookincubator| 9305|\n",
      "|facebookincubator|  998|\n",
      "|facebookincubator| 9304|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#test2 for 1st df \n",
    "org=spark.sql('Select username ,stars From lang WHERE username==\"facebookincubator\"').show(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the password for database from .env file\n",
    "load_dotenv()\n",
    "db_password = os.getenv(\"mysql_pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# writing 1st df into mysql db \n",
    "programming_lang.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306/Github_repos\") \\\n",
    "        .option(\"dbtable\", \"programming_lang_database\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", db_password) \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating 2nd df \n",
    "organizations_stars=spark.sql('Select username ,sum(stars) as total_number_of_stars From lang GROUP BY username')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+\n",
      "|        username|total_number_of_stars|\n",
      "+----------------+---------------------+\n",
      "|       tribbloid|                  239|\n",
      "|             P7h|                  386|\n",
      "|       zaratsian|                   68|\n",
      "|       jtnystrom|                    7|\n",
      "|            didi|                 4914|\n",
      "|          zdkzdk|                   49|\n",
      "|       zhangslob|                   43|\n",
      "|        deepmipt|                16875|\n",
      "|        blei-lab|                14079|\n",
      "|          GPflow|                 4786|\n",
      "|        asingh33|                  843|\n",
      "|        lambdaji|                  653|\n",
      "|        TheLordA|                   53|\n",
      "|         esoxjem|                 2571|\n",
      "|    ITDragonBlog|                 1163|\n",
      "|          anastr|                 1110|\n",
      "|       wewewe718|                  323|\n",
      "|        catchorg|                14637|\n",
      "|AtomicGameEngine|                 5249|\n",
      "|         p-ranav|                 5906|\n",
      "+----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#test for 2nd df \n",
    "organizations_stars.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[username: string, total_number_of_stars: bigint]\n"
     ]
    }
   ],
   "source": [
    "print(organizations_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#writing 2nd requistion into mysql db \n",
    "organizations_stars.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306/Github_repos\") \\\n",
    "        .option(\"dbtable\", \"organizations_stars_database\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\",db_password) \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['archive/Angular.json', 'archive/Django.json', 'archive/Docker.json', 'archive/Gatsby.json', 'archive/React-Native.json', 'archive/Tensorflow.json', 'archive/Cpp.json', 'archive/Scala.json', 'archive/Machine-Learning.json', 'archive/Kubernetes.json', 'archive/ethereum.json', 'archive/PyTorch.json', 'archive/serverless.json', 'archive/Scikit.json', 'archive/NextJS.json', 'archive/Deep-Learning.json', 'archive/Kotlin.json', 'archive/React-JS.json', 'archive/Julia.json', 'archive/NodeJS.json', 'archive/Golang.json', 'archive/Threejs.json', 'archive/R.json', 'archive/Typescript.json', 'archive/Dart.json', 'archive/Flask.json', 'archive/Spark.json', 'archive/OOPs.json', 'archive/Hadoop.json']\n"
     ]
    }
   ],
   "source": [
    "#getting all directories for the json files \n",
    "import os \n",
    "main_directory='archive'\n",
    "files=os.listdir(main_directory)\n",
    "list_of_directories=[]\n",
    "for file in files:\n",
    "    directory_of_file=os.path.join(main_directory, file)\n",
    "    list_of_directories.append(directory_of_file)\n",
    "\n",
    "print(list_of_directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Angular', 'Django', 'Docker', 'Gatsby', 'React-Native', 'Tensorflow', 'Cpp', 'Scala', 'Machine-Learning', 'Kubernetes', 'ethereum', 'PyTorch', 'serverless', 'Scikit', 'NextJS', 'Deep-Learning', 'Kotlin', 'React-JS', 'Julia', 'NodeJS', 'Golang', 'Threejs', 'R', 'Typescript', 'Dart', 'Flask', 'Spark', 'OOPs', 'Hadoop']\n"
     ]
    }
   ],
   "source": [
    "# getting names for json files \n",
    "df_names = [os.path.splitext(file)[0] for file in files]\n",
    "print(df_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creation for dfs for every file\n",
    "df_total_search=[]\n",
    "for i, directory in enumerate(list_of_directories):\n",
    "    search_term=df_names[i]\n",
    "    df=spark.read.json(directory)\n",
    "    df.createOrReplaceTempView('view_for_df')\n",
    "    relevance_score= spark.sql(f'SELECT \"{search_term}\" as search_term, sum(forks)*1.5 + sum(subscribers)*1.32 + sum(stars)*1.04 as relevance_score FROM view_for_df WHERE type==\"Organization\"')\n",
    "    df_total_search.append(relevance_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|search_term|relevance_score|\n",
      "+-----------+---------------+\n",
      "|    Angular|     1777052.40|\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_total_search[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 109:===============================================>       (25 + 4) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n",
      "|     search_term|relevance_score|\n",
      "+----------------+---------------+\n",
      "|         Angular|     1777052.40|\n",
      "|          Django|      898331.96|\n",
      "|          Docker|     2780901.88|\n",
      "|          Gatsby|      199343.26|\n",
      "|    React-Native|     1686382.14|\n",
      "|      Tensorflow|     1933000.62|\n",
      "|             Cpp|     2037344.12|\n",
      "|           Scala|      879416.38|\n",
      "|Machine-Learning|     3663187.06|\n",
      "|      Kubernetes|     2877257.56|\n",
      "|        ethereum|      898290.74|\n",
      "|         PyTorch|     1527742.68|\n",
      "|      serverless|     1011225.28|\n",
      "|          Scikit|      409319.20|\n",
      "|          NextJS|      399003.86|\n",
      "|   Deep-Learning|     3225845.64|\n",
      "|          Kotlin|      995196.54|\n",
      "|        React-JS|      412299.06|\n",
      "|           Julia|      281220.12|\n",
      "|          NodeJS|     2700362.96|\n",
      "|          Golang|     3074290.76|\n",
      "|         Threejs|       68310.94|\n",
      "|               R|     6804028.24|\n",
      "|      Typescript|     3741619.90|\n",
      "|            Dart|      548056.92|\n",
      "|           Flask|      386838.32|\n",
      "|           Spark|      230991.02|\n",
      "|            OOPs|        1627.82|\n",
      "|          Hadoop|      324775.30|\n",
      "+----------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#merging all dfs and names in one df \n",
    "merged_df = df_total_search[0]\n",
    "for df in df_total_search[1:]: \n",
    "    merged_df = merged_df.union(df)\n",
    "\n",
    "merged_df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#writing all dfs 3rd requistion into mysql db \n",
    "merged_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://localhost:3306/Github_repos\") \\\n",
    "        .option(\"dbtable\", \"Search_terms_relevance_database\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", db_password) \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
