{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, window, to_timestamp, when\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LogsMovingWindow\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_logs = df_raw.selectExpr(\"CAST(value AS STRING) as log_line\")\n",
    "\n",
    "pattern = r\"(\\d+\\.\\d+\\.\\d+\\.\\d+)\\s-\\s(\\d+)\\s\\[(.*?)\\]\\s(\\w+)\\s(\\S+)\\s(\\d+)\\s(\\d+)\"\n",
    "\n",
    "df_parsed = df_logs.select(\n",
    "    regexp_extract(\"log_line\", pattern, 1).alias(\"ip\"),\n",
    "    regexp_extract(\"log_line\", pattern, 2).alias(\"user_id\"),\n",
    "    regexp_extract(\"log_line\", pattern, 3).alias(\"timestamp\"),\n",
    "    regexp_extract(\"log_line\", pattern, 4).alias(\"method\"),\n",
    "    regexp_extract(\"log_line\", pattern, 5).alias(\"file\"),\n",
    "    regexp_extract(\"log_line\", pattern, 6).cast(\"int\").alias(\"status\"),\n",
    "    regexp_extract(\"log_line\", pattern, 7).cast(\"int\").alias(\"size\")\n",
    ")\n",
    "\n",
    "df_with_time = df_parsed.withColumn(\n",
    "    \"event_time\",\n",
    "    to_timestamp(\"timestamp\", \"EEE, dd MMM yyyy HH:mm:ss z\")  \n",
    ")\n",
    "\n",
    "df_classified = df_with_time.withColumn(\n",
    "    \"operation_type\",\n",
    "    when((col(\"method\") == \"GET\") & (col(\"status\") == 200), \"successful_get\")\n",
    "    .when((col(\"method\") == \"GET\") & (col(\"status\") != 200), \"failed_get\")\n",
    "    .when((col(\"method\") == \"POST\") & (col(\"status\") == 200), \"successful_post\")\n",
    "    .when((col(\"method\") == \"POST\") & (col(\"status\") != 200), \"failed_post\")\n",
    "    .otherwise(\"other\")\n",
    ")\n",
    "\n",
    "df_windowed_ordered = df_classified.groupBy(\n",
    "    window(\"event_time\", \"5 minutes\", \"5 minutes\"),\n",
    "    \"operation_type\"\n",
    ").count().withColumn(\"window_start\", col(\"window.start\"))\n",
    "\n",
    "df_sorted = df_windowed_ordered.orderBy(\"window_start\")\n",
    "\n",
    "query = df_sorted.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"checkpointLocation\", \"hdfs://namenode:9000/checkpoints/logs/\") \\\n",
    "    .option(\"path\", \"hdfs://namenode:9000/output/logs/\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
